# Creating and Using a Broker Instance for Testing

The following instructions will walk you through deploying an instance of the v0.2 broker code to GCP so that you can test the broker and its components.

See the repo's main [../README.md](../README.md) for a description of the broker architecture and links to the production resources.

Table of Contents:
- [Overview](#overview)
    - [Setup and utilize a _testing_ instance of the broker](#setup-and-utilize-a-testing-instance-of-the-broker)
    - [Setup a _production_ instance of the broker](#setup-a-production-instance-of-the-broker)
- [Detailed Workflow with code examples](#detailed-workflow-with-code-examples)
    - [Setup a testing instance](#setup-a-testing-instance)
    - [Run your tests](#run-your-tests)
        - [Example: Run the broker](#example-run-the-broker)
    - [Teardown the testing instance](#teardown-the-testing-instance)
    - [Leave the testing instance inactive](#leave-the-testing-instance-inactive)

## Overview

### Setup and utilize a _testing_ instance of the broker
<!-- fs -->
In this tutorial, we will create and deploy a _testing_ instance of the broker which you can play around with without touching any of the GCP resources or code that is running nightly in _production_.
We do this by creating new GCP resources (VMs, Pub/Sub streams, buckets, etc.) from the source code in this branch, which have their names tagged with a user-defined string, the "testid".
Switches have been placed throughout the code which connect resources that are tagged with the same testid.
For example, _if we create and deploy a testing instance of the broker with the testid "mytest":_
- the `ztf-consumer` VM will be named `ztf-consumer-mytest`,
    - it will publish alerts to the Pub/Sub topic named `ztf_alert_data-mytest`
- the downstream components (e.g., Dataflow jobs) will have similarly tagged names,
    - they will get their input data from the `ztf_alert_data-mytest` topic,
    - and they will publish to resources with similarly tagged names.

You can use this testing instance to test/alter/play around with any component of the broker.
In this tutorial we will use it to test the broker setup scripts, the nightly deployment scripts, and the consume/process pipelines.
In the future, we can use testing instances to:
- debug (e.g., alter the startup script used by the `ztf-consumer-{testid}` VM and test the effect.)
- run unit tests (e.g., I anticipate that we should configure Travis to build/use a testing instance with the testid "travis")
- develop and test a new science component (e.g., ML classifier) for the value-added pipeline.

I think it should be straightforward to configure a _testing_ instance to connect to some component(s) of the _production_ broker (e.g., connecting a testing instance of a Dataflow job to the live `ztf_alert_data` Pub/Sub stream generated by the production consumer)
by altering or simply removing the relevant switch(es) that tag the name(s) of testing resource(s) with the testid.

If you are not actively using your testing instance, but you are not ready to delete it,
be sure to _shutdown all VMs and stop/drain all Dataflow jobs_ (these are the most expensive components to leave running when not in use).
See the specific code below, but in general the simplest way to do this to trigger the `night-conductor` VM to end the night.

When you are done with it, please teardown (delete) your testing instance of the broker.
See below for the specific code, but essentially you execute the `setup_broker.sh` script with the `teardown` flag set to `True`.

#### Requirements on `testid`
- must be a short _string_ (this gets appended to resource names, which have length limits)
- must _start with_ a lowercase letter
- _may_ contain __*lowercase letters and numbers*__
- may _not_ contain capital letters
- may _not_ contain any other characters including spaces, "-", "_", "/", etc.

<!-- fe Create, deploy, and utilize a testing instance of the broker -->

### Setup a _production_ instance of the broker
<!-- fs -->
Strictly speaking, this tutorial is not intended to cover this topic, but the only difference between a testing and production instance of the broker is the presence/absence of a tag (string) appended to the names of all GCP resources that created for, and used by, the broker instance.
So, I will cover some relevant items here.

The production instance only needs to be _setup_ once and it has already been done.

Most components of the broker can be _updated_ by simply replacing the relevant files in the `broker-files` GCS bucket, which has been set up to stage the files used to run the nightly broker.
This can be done manually, or by re-running the setup using `setup_broker.sh`.
By design, the setup script(s) will skip the _creation_ of any resources that already exist, but it will still upload new broker files to the staging bucket, overwriting any existing files.

To __setup the production broker__ the process is exactly the same as for a testing instance, except that we set the testid to `False`.
This builds and uses GCP resources that do _not_ have a testid appended to the names.

The "teardown" options that can be triggered in the setup scripts are specifically configured to _prevent_ a user from deleting _production_ resources, even if the user intentionally tries to do so.
This is one layer of security, but we should still look into protecting those resources in other ways (e.g., make backups of databases and buckets, find out which resources can be configured directly to prevent deletion and configure them).

<!-- fe Create and deploy a production instance of the broker -->


## Detailed Workflow with code examples
<!-- fs -->
To test the broker, you should perform these 3 steps:
1. __Setup__ your own instance of the broker for testing.
2. __Run your tests__ using the instance you created in step 1.
3. __Teardown__ the broker instance when you are done (i.e., delete the GCP resources created in step 1)

You can also leave the testing instance inactive, just be sure to shutdown all VMs and end all Dataflow jobs.

### Setup a testing instance
<!-- fs -->
If you don't have GCP command line tools like `gcloud` and `gsutil` installed, follow the instructions in step 2 of [../README.md](../README.md#setup-the-broker-for-the-first-time).
Activate your virtual environment if needed.

```bash
#--- Set the project ID env variable if it's not already
export GOOGLE_CLOUD_PROJECT=ardent-cycling-243415

#--- The Compute Engine VMs must be assigned to a specific zone.
# We currently use the same zone for all instances.
# The default is us-central1-a,
# but it can controlled explicitly by setting an environment variable
export CE_zone=us-central1-a

#--- Get the current broker repo/branch and navigate to the setup directory
git clone https://github.com/mwvgroup/Pitt-Google-Broker
git fetch
git checkout u/tjr/broker-v0.2
git pull
cd Pitt-Google-Broker/broker/setup_broker

#--- Setup a testing instance of the broker, tagged with "mytest"
testid="mytest"
./setup_broker.sh $testid
# this script is described in the text below
```

Resources linked below are _production_ resources.
The testing resources you create will have the same names with your chosen testid appended (and joined by either "-" or "_").

`setup_broker.sh` does the following:

1. Create and configure GCP resources in BigQuery, Cloud Storage, and Pub/Sub.

2. Upload the [beam](beam/), [consumer](consumer/), and [night_conductor](night_conductor/) directories to the Cloud Storage bucket [ardent-cycling-243415-broker_files](https://console.cloud.google.com/storage/browser/ardent-cycling-243415-broker_files?project=ardent-cycling-243415&pageState=%28%22StorageObjectListTable%22:%28%22f%22:%22%255B%255D%22%29%29&prefix=&forceOnObjectsSortingFiltering=false). The VMs will fetch a new copy of these files before running the relevant process. This provides us with the flexibility to update individual broker processes/components (except Cloud Functions) by simply uploading a new version of the relevant file(s) to the bucket; we do not have to build a new Docker image, repackage, or redeploy.

3. Configure Pub/Sub notifications (topic [`ztf_alert_avro_bucket`](https://console.cloud.google.com/cloudpubsub/topic/detail/ztf_alert_avro_bucket?project=ardent-cycling-243415)) on the GCS bucket that stores the alert Avro files (bucket [`ztf_alert_avro_bucket`](https://console.cloud.google.com/storage/browser/ardent-cycling-243415_ztf_alert_avro_bucket;tab=objects?forceOnBucketsSortingFiltering=false&project=ardent-cycling-243415&prefix=&forceOnObjectsSortingFiltering=false)).

4. Create a VM firewall rule to open the port used by ZTF's Kafka stream.
This step will _fail_ because the rule already exists and we don't need a separate rule for testing resources.
_You can ignore it._

5. Deploy the Cloud Function [`upload_ztf_bytes_to_bucket`](https://console.cloud.google.com/functions/details/us-central1/upload_ztf_bytes_to_bucket?project=ardent-cycling-243415&pageState=%28%22functionsDetailsCharts%22:%28%22groupValue%22:%22P1D%22,%22customValue%22:null%29%29) which stores alerts as Avro files in the Cloud Storage bucket [`ztf_alert_avro_bucket`](https://console.cloud.google.com/storage/browser/ardent-cycling-243415_ztf_alert_avro_bucket;tab=objects?forceOnBucketsSortingFiltering=false&project=ardent-cycling-243415&prefix=&forceOnObjectsSortingFiltering=false).

6. Create and configure the Compute Engine instances
    - [`night-conductor`](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/night-conductor?tab=details&project=ardent-cycling-243415): we use this VM to start and stop the nightly broker.
    - [`ztf-consumer`](https://console.cloud.google.com/compute/instancesMonitoringDetail/zones/us-central1-a/instances/ztf-consumer?project=ardent-cycling-243415&tab=monitoring&duration=PT1H&pageState=%28%22duration%22:%28%22groupValue%22:%22P7D%22,%22customValue%22:null%29%29): this VM consumes ZTF's Kafka streams and publishes the alerts to the Pub/Sub topic [`ztf_alert_data`](https://console.cloud.google.com/cloudpubsub/topic/detail/ztf_alert_data?project=ardent-cycling-243415).


The consumer VM (`ztf-consumer-mytest` in our example) requires two __authorization files__ to connect to the ZTF stream.
_These must be obtained independently and uploaded to the VM manually, stored at the following locations:_
    1. `krb5.conf`, at VM path `/etc/krb5.conf`
    2. `pitt-reader.user.keytab`, at VM path `/home/broker/consumer/pitt-reader.user.keytab`

You can use the `gcloud compute scp` command for this:
```bash
gcloud compute scp /local/path "ztf-consumer-${testid}:/vm/path" --zone="$CE_zone"
```

<!-- fe Setup a testing instance -->

### Run your tests
<!-- fs -->
You can alter your testing instance of the broker at-will, and use it to execute any type of testing that you'd like.

Here, we use it to walk through the process of actually running the broker to ingest/store/process the ZTF alert stream.

#### Example: Run the broker

The broker employs the [`night-conductor`](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/night-conductor?tab=details&project=ardent-cycling-243415) VM to orchestrate starting and ending the night.
Its _startup script_, [night_conductor/vm_startup.sh](night_conductor/vm_startup.sh), contains the required logic to start and stop GCP resources.
(Note that the startup script is staged in the bucket [ardent-cycling-243415-broker_files](https://console.cloud.google.com/storage/browser/ardent-cycling-243415-broker_files?project=ardent-cycling-243415&pageState=%28%22StorageObjectListTable%22:%28%22f%22:%22%255B%255D%22%29%29&prefix=&forceOnObjectsSortingFiltering=false) along with other files used to run the broker.)

To control the behavior of the script, we pass arguments to it by setting _metadata attributes_ on `night-conductor` prior to starting the VM.
Metadata attributes are cleared before the VM shuts down so that no unexpected behavior occurs on the next startup.

To start the broker for the night, we must set a metadata attribute with the name of the _ZTF/Kafka topic_ the consumer should subscribe to.
ZTF publishes alerts to a new topic each night, and our consumer's connection to the Kafka stream will fail if there is not at least one alert in the topic.
The syntax for the topic name is `ztf_yyyymmdd_programid1`.
If you want to test this during the day, you can connect to any topic _within the last 7 days_ that contains at least 1 alert.
You can check [ztf.uw.edu/alerts/public/](https://ztf.uw.edu/alerts/public/).
`tar` files larger than 74 (presumably in bytes) indicate dates with >0 alerts.
See below for what to expect if you let it consume an active night's worth of alerts as fast as it can.

```bash
testid=mytest
instancename="night-conductor-${testid}"
zone=us-central1-a
broker_bucket="${GOOGLE_CLOUD_PROJECT}-broker_files-${testid}"

#--- Start the night
# set the attributes
NIGHT=START
KAFKA_TOPIC=ztf_20210120_programid1 # ztf_yyyymmdd_programid1
# must be within the last 7 days and contain at least 1 alert
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
        --metadata NIGHT="$NIGHT",KAFKA_TOPIC="$KAFKA_TOPIC"

# the startup script should already be set, but we can make sure
startupscript="gs://${broker_bucket}/night_conductor/vm_startup.sh"
gcloud compute instances add-metadata "$instancename" --zone "$zone" \
        --metadata startup-script-url="$startupscript"

# start the VM to trigger the startup script
gcloud compute instances start "$instancename" --zone "$zone"
# night-conductor will get the testid by parsing its own instance name
# night-conductor shuts down automatically when broker startup is complete

#--- The broker will start ingesting the ZTF stream
#    and storing/processing the data.

#--- End the night
# set the attributes
NIGHT=END
gcloud compute instances add-metadata "$instancename" --zone="$zone" \
      --metadata NIGHT="$NIGHT"

# start the VM to trigger the startup script
gcloud compute instances start "$instancename" --zone "$zone"
# night-conductor will get the testid by parsing its own instance name
# the night-conductor VM shuts down automatically when broker shutdown is complete
```

__Start Night Details:__

Currently, `night-conductor` executes the following to start the night:
1. Clears the messages from Pub/Sub subscriptions that we use to count the number of elements received and processed each night.
2. Deploys the two Dataflow jobs and waits for their status to change to "Running".
3. Starts the [`ztf-consumer`](https://console.cloud.google.com/compute/instancesMonitoringDetail/zones/us-central1-a/instances/ztf-consumer?project=ardent-cycling-243415&tab=monitoring&duration=PT1H&pageState=%28%22duration%22:%28%22groupValue%22:%22P7D%22,%22customValue%22:null%29%29) VM, which is configured with a startup script to connect to ZTF and begin ingesting.

Cloud Functions are always "on"; `night-conductor` does not manage them.

Based on my experience, _what to expect if you allow the broker to ingest a ZTF topic that already contains 200,000 - 400,000 alerts (a typical, active night) as fast as it can_:
- The consumer should ingest and publish all of the alerts to the `ztf_alert_data` Pub/Sub topic within about 30 minutes.
- The Avro->GCS storage component (Cloud Function) should process all of the alerts within about 60 minutes.
- The BigQuery storage component (Dataflow job) will likely get bogged down and take >10 hours to process all of the alerts. I recommend just cancelling the job. This component performs reasonably at the live-stream rate, but it tends to struggle when flooded with alerts. In general, it is the component with the largest lag times and number of dropped alerts. It could use some TLC, or a complete reconfiguring.
- The value-added processing component (Dataflow job) should process all of the alerts within a few hours.

Note on triggering `night-conductor` to start the night:
The consumer's connection to ZTF will fail if there is not as least 1 alert in the topic, but the terminal/shell is not released,
so we can't simply keep trying until the connection succeeds.
Therefore I am still manually triggering `night-conductor` to start the night after ZTF issues its first alert.
There _should_ be a programatic way to check whether a topic is available, but I haven't been able to find it yet.
I have a new lead, so I'll work on it more.


__End Night Details:__

Currently, `night-conductor` executes the following to end the night:
1. Stop the [`ztf-consumer`](https://console.cloud.google.com/compute/instancesMonitoringDetail/zones/us-central1-a/instances/ztf-consumer?project=ardent-cycling-243415&tab=monitoring&duration=PT1H&pageState=%28%22duration%22:%28%22groupValue%22:%22P7D%22,%22customValue%22:null%29%29) VM (which stops the ingestion).
2. Drains the Dataflow jobs (it stops accepting new alerts, finishes the processing of all alerts in the pipeline, and the job ends).

Note on triggering `night-conductor` to end the night:
There is no programatic way to check whether ZTF has sent its last alert for the night,
or to check whether we have received all the alerts ZTF has sent.
Christopher Phillips at ZTF says we can assume that ZTF has sent out all of its alerts by shortly after sunrise ZTF time.
I am still manually triggering `night-conductor` to end the night.
Over the last ~2 months (today is 1/19/21), ZTF has consistently been done issuing alerts by 9:30am ET, and our broker does not have a significant lag.
I plan to automate triggering `night-conductor` to end the night at 10am ET, but this will need to be adjusted seasonally.

<!-- fe Run your tests -->

### Teardown the testing instance
<!-- fs -->
When you are done with your testing instance of the broker, delete it by running the setup script with the "teardown" argument.

```bash
cd Pitt-Google-Broker/broker/setup_broker

# Teardown all test resources with the testid "mytest"
testid="mytest"
teardown="True"
./setup_broker.sh $testid $teardown
```

This will delete all GCP resources tagged with the testid.

<!-- fe Teardown the testing instance -->

### Leave the testing instance inactive
<!-- fs -->
You can leave the testing resources inactive, but please shutdown all VMs and stop all Dataflow jobs so that we don't incur a cost for them to sit idle.
If you followed the example above to run the broker, triggering the night conductor to end the night will accomplish this.
To do it manually:

```bash
# stop both VMs
testid=mytest
consumerVM="ztf-consumer-${testid}"
nconductVM="night-conductor-${testid}"
zone=us-central1-a
gcloud compute instances stop "$consumerVM" "$nconductVM" --zone="$zone"
```

To stop/drain/cancel a Dataflow job from the command line, you would need to look up the job ID assigned by Dataflow at runtime.
It's easier to stop the job manually from the [Dataflow console](https://console.cloud.google.com/dataflow/jobs?project=ardent-cycling-243415).

<!-- fe Leave the testing instance inactive -->

<!-- fe Detailed Workflow with code examples -->
