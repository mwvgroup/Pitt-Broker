{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data to Big Query\n",
    "\n",
    "This notebook downloads data from the ZTF Public Alerts Archive and uploads it into big query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from broker import ztf_archive as ztfa\n",
    "from broker.alert_acquisition.ztf import map_alert_list_to_schema\n",
    "from broker.data_upload import batch_ingest\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by retrieving a list of available ZTF releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list = ztfa.get_remote_release_list()\n",
    "release_list.pop()  # Remove checksums file\n",
    "print(release_list[:5])  # Print the first 5 for demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the year, month, and date for each release. Here we only keep releases from December 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_dates = []\n",
    "for release in release_list:\n",
    "    date_str = release.strip('ztf_public_').strip('.tar.gz')\n",
    "    date_tuple = (int(date_str[:4]), int(date_str[4:6]), int(date_str[6:]))\n",
    "    if date_tuple[0] == 2018 and date_tuple[1] == 12:\n",
    "        alert_dates.append(date_tuple)\n",
    "\n",
    "print(f'Number of releases: {len(alert_dates)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be uploading data iterativly, we expect a few upload events to time out or fail for other seemingly \"random\" reasons. We thus define an `upload` function to help handel these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload(data, data_set, table_name, max_tries=5):\n",
    "    \"\"\"Batch upload a Pandas DataFrame into a BigQuery table\n",
    "\n",
    "    If the upload fails, retry until success or until \n",
    "    max_tries is reached.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Data to upload to table\n",
    "        data_set   (str): The name of the data set\n",
    "        table      (str): The name of the table\n",
    "        max_tries  (int): Maximum number of tries until error\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i >= max_tries:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            batch_ingest(data, data_set, table_name)\n",
    "            return\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error uploading to table {table_name}: {str(e)}')\n",
    "            print('Trying again...')\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    raise RuntimeError('Could not upload data.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we download data from ZTF and upload it into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for date in alert_dates:\n",
    "    \n",
    "    try:\n",
    "        ztfa.download_data_date(*date)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    print('Uploading to Big Query.')\n",
    "    total_iter = np.ceil(len(ztfa.get_local_alert_list()) / 800)\n",
    "    iter_data = ztfa.iter_alerts(800)\n",
    "    for data in tqdm(iter_data, total=total_iter):\n",
    "        alert_df, candidate_df = map_alert_list_to_schema(data)\n",
    "        upload(alert_df, 'ztf_alerts', 'alert')\n",
    "        upload(candidate_df, 'ztf_alerts', 'candidate')\n",
    "    \n",
    "    print('Deleting Local Data.')\n",
    "    shutil.rmtree(ztfa._download_data.DATA_DIR)\n",
    "    os.mkdir(ztfa._download_data.DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding data into BigQuery, it's a good idea to check for duplicate data. To select only the unique columns, use the following SQL statement in BigQuery.\n",
    "\n",
    "```\n",
    "SELECT * EXCEPT(row_number)\n",
    "FROM (\n",
    "  SELECT\n",
    "      *,\n",
    "      ROW_NUMBER()\n",
    "          OVER (PARTITION BY candID)\n",
    "          row_number\n",
    "  FROM ztf_alerts.alert\n",
    ")\n",
    "WHERE row_number = 1;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pitt_broker] *",
   "language": "python",
   "name": "conda-env-pitt_broker-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
